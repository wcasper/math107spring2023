---
layout: post
---

Notes and highlights from lecture

## Today's Objectives

* Principal component analysis
* Singular value decomposition

## Principal Component Analysis

A problem that arises often in the real world is how to sort through a huge collection of noisy and potentially irrelevant measurements and data to get at the one or two measurements which are the most important.

Take for example the problem of Netflix movie recommendations.
Netflix has a huge amount of information about the watching habits of their users, including when they watch, what they watch, and how long they watch it, not to mention all the ratings that users themselves submit into their own accounts.
Furthermore, each movie itself has a ton more data, such as the genre, the director, the actors/actresses, and the plot.
Out of all of this data, what factors are the most important in determining whether Netflix should recommend a movie to a particular customer?

The frequency and importance of problems like this one have led to many different strategies and algorithms for determining good answers.  One of the most popular methods is Principal Component Analysis (PCA).

In PCA, we start with an $$n\times p$$ matrix of data $$X$$, which can be thought of as experimental data from $$n$$ experiments, where during any particular experiment we measure $$p$$ many things.  For example, we can imagine we ran an experiment where we collected two values $$x$$ and $$y$$ each experiment.  Then we can plot all of our experiments as blue points in a graph below.

<center>
<img src="/math107spring2022/extras/img/pca1.png" alt="sample data plot" width="300">
</center>

The red point featured above is the average value over all experiments.
In MATLAB, we can represent our experimental data as a matrix $$X$$, where each row of the matrix is a different experiment and the entries of a row are the observations we found for a given experiment.  Thus for the picture above, $$X$$ will be an $$n\times 2$$ matrix, where $$n$$ is the number of blue dots.

Next, we perform the important step of **removing the mean** of the data, creating a new matrix $$Y$$ whose rows are the rows of $$X$$, except that we have subtracted the average of all the row vectors from each row.  In MATLAB, we can use the code

```Matlab
Y = X - mean(X,1)
```

Graphically, this translates the data so that it is centered at the origin, as pictured below.

<center>
<img src="/math107spring2022/extras/img/pca2.png" alt="sample data plot" width="300">
</center>

Lastly, we create the **covariance matrix**  $$C = Y^TY$$, which will be a $$p\times p$$ matrix (a $$2\times 2$$ matrix in our example).  In MATLAB, this can be done with the code.

```Matlab
C = tranpose(Y)*Y
```

The eigenvalues and eigenvectors of the matrix $$C$$ tell us a lot about how the data is spread out.  For example, in our sample data above the data seems to be stretched a lot in one direction and very flat in another.  This is also shown by the eigenvectors and eigenvalues, which we have drawn into the picture below.

<center>
<img src="/math107spring2022/extras/img/pca3.png" alt="sample data plot" width="300">
</center>

The eigenvector $$\vec v$$ with the biggest eigenvalue reveals the direction where most of the data is stretched.  The remaining eigenvector $$\vec u$$ has a much smaller eigenvalue and represents the direction where the data is squashed.  

**Big Observation:** The direction of the eigenvector with biggest eigenvalue reveals the most important feature or pattern to the data.

### Summary

To summarize, the PCA algorithm is the as follows.

**PCA Algorithm Steps**

* (1) Create a matrix $$Y$$ whose columns are the columns of $$X$$ minus their mean values.  In MATLAB this can be done using

```Matlab
Y = X - mean(X,1)
```

* (2) Create the **covariance matrix** $$C = Y^TY$$.  In MATLAB this can be done using

```Matlab
C = tranpose(Y)*Y
```

* (3) Calculate the eigenvectors and eigenvalues of the covariance matrix.  In MATLAB this can be done using

```MATLAB
[P,D] = eig(C)
```

* (4) The eigenvector with the largest eigenvalues correspond to the most important measurement, the **principal component** of the system.  Assuming that this eigenvalue is much larger than the other eigenvalues, this means that the most important measurement in the system is the dot product of each row vector of $$Y$$ with the associated eigenvector.
In other words, by knowing that given linear combination of measurements for a particular experiment, we could strongly guess what each of the measurements were individually.

## Mass-Spring System Example

For example imagine that in a certain experiment, we attach various weights to a spring and see how far the spring stretches past its equilibrium length.  Of course, in the actual measurements there are errors in both the measurements of the weights attached and the measurements of how far the spring is stretched.
Below is the data recorded in the lab

|              | weight attached (lbs) | stretch length (ft) |
| ------------ | --------------------- | ------------------- |
| experiment 1 |           1.0         |       0.009         |
| experiment 2 |           1.2         |       0.013         |
| experiment 3 |           1.4         |       0.014         |
| experiment 4 |           1.6         |       0.017         |
| experiment 5 |           1.8         |       0.018         |
| experiment 6 |           2.0         |       0.019         |

Using PCA we can (without any knowledge of physics), determine the most important factors contributing to change in the system.

For the spring experiment above, we calculate

$$C = \left[\begin{array}{cc}
0.70000 & 0.00680\\
0.00680 & 0.00007
\end{array}\right].$$

Using MATLAB, we calculate that the eigenvalues of this matrix are $$0.7000661$$ and $$0.0000039$$, so the principal component of this system corresponds to the larger eigenvalue.  An eigenvector with this eigenvalue is calculated to be $$\binom{0.9999528}{0.00971388}$$.  The first entry corresponds to the width measurement, while the second measurement corresponds to the stretch length.  This means that the most important measurement of the system is the component in the direction of this eigenvector!

If we let $$\overline{w}$$ and $$\overline{\ell}$$ be the average values of the weight $$w$$ and the length $$\ell$$ in the table above, this simply says

$$\binom{w-\overline{w}}{\ell-\overline{\ell}} \approx \alpha\binom{0.9999528}{0.00971388}$$

for some constant $$\alpha$$.  In other words $$\binom{w}{\ell}$$ should be approximately the same direction of this eigenvector!  Thus approximately

$$w-\overline{w} \approx 0.9999528\alpha,\quad\text{and}\quad \ell-\overline{\ell} \approx 0.00971388\alpha,$$

for some unknown constant $$\alpha$$.  After a little algebra, we can get rid of this constant to find

$$\ell-\overline{\ell} \approx \frac{0.00971388}{0.9999528}(w-\overline{w}).$$

Also, we can calculate that $$\overline w = 1.5$$ and $$\overline \ell = 0.015$$ so that

$$\ell \approx \frac{0.00971388}{0.9999528}w + 0.0004223.$$

This is the equation of a straight line!
If we plot our measurements along with the line corresponding to the principal
component, we see that the measurements all lie very close to the line, as
shown in the figure below.

![Spring PCA](/math107spring2022/extras/img/springPCA.jpg)

In actuality, we have rediscovered **Hooke's law** that the stretch length is proportional to the weight added, via some proportionality constant called the **spring constant** which differs from spring to spring:

$$\textbf{Hooke's Law:}\quad \ell = kw.$$

For our spring, the eigenvector tells us the spring constant should be $$k= 0.00971388/0.9999528 = 0.0097143404$$.  The additional $$0.0004223$$ in our model can be attributed to measurement error.

## Understanding Housing Prices

Another application of PCA is to understand what variables most strongly impact the cost of a house.  What is it that most strongly influences the price?  The square footage, the number of bathrooms, the number of bedrooms, or some interesting combination?  How important are each of the relative quantities in predicting the cost?

As an example, consider the data set below featuring the costs of houses in a particular area, along with some data on each house.


| Cost $$C$$ | Interior square footage $$x$$ | Exterior square footage $$z$$ | Bathrooms $$m$$ | Bedrooms $$n$$ |
| ---------- | ----------------------------- | ----------------------------- | --------------- | -------------- |
|  234641    |               1000            |               3000            |         2       |        1       |
|  449749    |               2125            |                  0            |         1.5     |        2       |
|  648350    |               3025            |               1000            |         2       |        3       |
|  352657    |               1500            |               2100            |         3       |        2       |
|  592788    |               2800            |                500            |         2       |        2       |
|  427948    |               1900            |                900            |         2       |        3       |


We can input this as a matrix, and run PCA to find that the largest eigenvalue of the covariance matrix is $$1.160725$$ with eigenvector

$$\vec v = \left[\begin{array}{c}
   0.999973636055984\\
   0.005026183374745\\
  -0.005240673775691\\
  -0.000000842031589\\
   0.000003365536509
\end{array}\right]$$

Then we expect 

$$
\left[\begin{array}{c} C-\overline{C} \\ x - \overline{x} \\ z - \overline{z} \\ m -\overline{m} \\ n - \overline{n}
\end{array}\right]
\approx\alpha
\left[\begin{array}{c}
   0.999973636055984\\
   0.005026183374745\\
  -0.005240673775691\\
  -0.000000842031589\\
   0.000003365536509
\end{array}\right]$$

In particular, the eigenvector gives us a way to predict the cost directly from the interior square footage, exterior square footage, bathrooms, and bedrooms:

$$C-\overline{C} = v_1\frac{v_2(x-\overline{x}) + v_3(z-\overline{z}) + v_4(m-\overline{m}) + v_5(n-\overline{n})}{v_2^2+v_3^2+v_4^2+v_5^2},$$

where $$v_1,\dots, v_5$$ are the components of the eigenvector $$\vec v$$.

## Singular Value Decomposition

Principal component analysis is very related to another idea, called **singular value decomposition**, or SVD.
In SVD, an $$m\times n$$ matrix $$A$$ is written as a product

$$A = UDV^T$$

for $$D$$ an $$m\times n$$ diagonal matrix, $$U$$ an $$m\times m$$ matrix and $$V$$ an $$n\times n$$ matrix satisfying $$U^T = U^{-1}$$ and $$V^T=V^{-1}$$.

Note that the matrices $$U$$ and $$V$$ are called unitary.

**Definition:** A matrix $$U$$ is called unitary if $$U^T=U^{-1}$$.

The process of calculating the SVD is as follows

**SVD Algorithm Steps**

* (1) Find an $$n\times n$$ unitary matrix $$V$$ whose columns are eigenvectors of $$A^TA$$.  In MATLAB this can be done by

```Matlab
[V,F] = eig(transpose(A)*A);
```

* (2) Find an $$m\times m$$ unitary matrix $$U$$ whose columns are eigenvectors of $$AA^T$$.  In MATLAB this can be done by

```Matlab
[U,E] = eig(A*transpose(A));
```

* (3) Set $$D = U'*A*V$$, which should be an $$m\times n$$ diagonal matrix whose entries are the eigenvalues we calculated above


PCA can be viewed as equivalent to SVD when things are examined carefully.
Moreover, SVD has the direct application of allowing for low rank approximations of a specific matrix $$A$$ by simply making some of the entries of $$D$$ equal to zero.
This has many applications, such as image compression.

## Reading assignments

* <a target="_parent" href="../../../extras/textbook.pdf">primary text (link)</a>: chapter 12 Section 3
* When Life Is Linear: Ch. 7 (14 pages)

## Additional resources




